{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNB7UHdrnIkt93g0/0DzQbF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JapiKredi/RAG_assignment_Research_papers/blob/main/RAG_assignment_Research_papers_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment \"Course 3 Mr.HelpMate\"\n",
        "### Learner: Jasper Bongers\n",
        "#### Option 1: Build Your Own Project (BYOP)\n",
        "\n",
        "- - I a building a generative AI system that is reading 5 popular Research Papers in Natural Language Understanding.\n",
        "I am building a chat bot that is doing generative search thru 9 very popular and important AI Research Papers on Large Language Models.\n",
        "- I am using 3 layers: the emebdding layer, the search layer, and the generation layer.\n",
        "- My goal in this session is to build a robust search system that can answer user queries effectively by experimenting with the various blocks in the system, such as chunking strategies, embedding models, re-rankers, generation prompt.\n",
        "\n",
        "- The top-9 great research papers are:\n",
        "1) ALiBi: TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR\n",
        "BIASES ENABLES INPUT LENGTH EXTRAPOLATION.\n",
        "2) Attention Is All You Need.\n",
        "3) Augmented Language Models: a Survey.\n",
        "4) FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awarenes\n",
        "5) Gemini: A Family of Highly Capable Multimodal Models.\n",
        "6) GPT-4 Technical Report\n",
        "7) LLaMA: Open and Efficient Foundation Language Models\n",
        "8) REACT: SYNERGIZING REASONING AND ACTING IN\n",
        "LANGUAGE MODELS\n",
        "9) Toolformer: Language Models Can Teach Themselves to Use Tools\n",
        "\n"
      ],
      "metadata": {
        "id": "nrZb6hRCTDKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Introduction and Setup:\n",
        "We will explore a multi-document retriever using ChromaDB for the database and vector store.\n",
        "The focus will be on implementing locally run embeddings, ideally on a GPU.\n",
        "\n",
        "2) Embedding Selection:\n",
        "Two options for embeddings will be introduced: standard Hugging Face embeddings (using models like sentence transformers) and custom instructor embeddings.\n",
        "The latter, specifically the XL variety, will be chosen for its apparent effectiveness.\n",
        "\n",
        "3) Data Format Transition:\n",
        "Instead of text files, we will work with multiple PDF files, featuring papers related to recent large language model discussions.\n",
        "\n",
        "4) Document Loading and Processing:\n",
        "Documents will be loaded using a simple pyPDF loader.\n",
        "The splitting and processing steps will remain consistent, maintaining simplicity.\n",
        "\n",
        "5) Embedding Process:\n",
        "Embeddings will be obtained using the selected method (instructor embeddings), run locally on a GPU.\n",
        "Configuration options will include choosing between local GPU or CPU processing, with acknowledgment of the trade-off in processing time.\n",
        "\n",
        "6) Vector Store Setup:\n",
        "The vector store will be established similarly to the previous approach, but with the integration of the new instructor embeddings.\n",
        "A directory will be persisted, and the vector store will be created from documents using the instructor embeddings and document text.\n",
        "\n",
        "7) Retriever Configuration:\n",
        "The retriever will be configured to utilize the new embeddings, enabling it to find contextually relevant documents based on queries.\n",
        "\n",
        "8) Chain Construction:\n",
        "A chain will be constructed, involving the retriever for the vector store and embeddings.\n",
        "Additional code will be added to neatly wrap the answers obtained from the retriever.\n",
        "\n"
      ],
      "metadata": {
        "id": "0vBdm0DAlq-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = green> Solution 1\n"
      ],
      "metadata": {
        "id": "M1uGjmwabl0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. <font color = red> Install and Import the Required Libraries"
      ],
      "metadata": {
        "id": "3yJccHFLZlzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please run the following code on a GPU, as running it on a CPU can create a crash"
      ],
      "metadata": {
        "id": "DO9Lii3UWn0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all the right dependencies\n",
        "!pip -q install langchain openai tiktoken chromadb pypdf sentence_transformers InstructorEmbedding"
      ],
      "metadata": {
        "id": "tM2SfdrpmahI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all the required Libraries\n",
        "import openai\n",
        "import pypdf\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from operator import itemgetter\n",
        "import json\n",
        "import tiktoken\n",
        "import chromadb"
      ],
      "metadata": {
        "id": "iM66u5VOaOH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all the required Libraries and modules\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
        "from sentence_transformers import CrossEncoder, util"
      ],
      "metadata": {
        "id": "H1iXzS2RmRkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check on Langchain\n",
        "!pip show langchain"
      ],
      "metadata": {
        "id": "m4EaVMgcal1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. <font color = red> Read, Process, and Chunk the PDF Files"
      ],
      "metadata": {
        "id": "qFh75N0obVh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "q_ShunTrbNCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the API key\n",
        "filepath = \"/content/drive/My Drive/GenerativeAI/MateAI/\"\n",
        "\n",
        "with open(filepath + \"Jasper_OpenAI_API_Key.txt\", \"r\") as f:\n",
        "  openai.api_key = ' '.join(f.readlines())"
      ],
      "metadata": {
        "id": "cdxqTb2nbYwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the API key\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ],
      "metadata": {
        "id": "uHeJhqM_bkcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load multiple and process documents"
      ],
      "metadata": {
        "id": "zw3q2Z7WbrGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and process the text files\n",
        "loader = DirectoryLoader(\"/content/drive/My Drive/GenerativeAI/MateAI/rag_assignment/research_articles/\", glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "SZDEVtz4bnUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "id": "thmX1hn9bvN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting, Chunking of text"
      ],
      "metadata": {
        "id": "8ikbCPH4b0tD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the text into\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "6Ow5Rtzgbyhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "id": "vGQvZRvXb6d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[10]"
      ],
      "metadata": {
        "id": "Y0BtAdMHb8ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding the text via regular Hugging Face process"
      ],
      "metadata": {
        "id": "E-ZfgB8fcCNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the HuggingFaceInstructEmbeddings via Hugging Face\n",
        "\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "\n",
        "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\",\n",
        "                                                      model_kwargs={\"device\": \"cuda\"})"
      ],
      "metadata": {
        "id": "cDXv53MXh7WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Chroma Vector Database"
      ],
      "metadata": {
        "id": "6sb5UvObN5Z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path where chroma collections will be stored\n",
        "chroma_data_path = '/content/drive/My Drive/GenerativeAI/MateAI/rag_assignment/ChromaDB_Data'"
      ],
      "metadata": {
        "id": "HoWd-s_JiiEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed and store the texts\n",
        "# Supplying a persist_directory will store the embeddings on disk\n",
        "persist_directory = chroma_data_path\n",
        "\n",
        "## Here is the nmew embeddings being used\n",
        "embedding = instructor_embeddings\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=texts,\n",
        "                                 embedding=embedding,\n",
        "                                 persist_directory=persist_directory)"
      ],
      "metadata": {
        "id": "vEURA2ticTkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# persiste the db to disk\n",
        "vectordb.persist()\n",
        "vectordb = None"
      ],
      "metadata": {
        "id": "KzIh5CScgFs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can load the persisted database from disk, and use it as normal.\n",
        "vectordb = Chroma(persist_directory=persist_directory,\n",
        "                  embedding_function=embedding)"
      ],
      "metadata": {
        "id": "GTYeD4cwgi_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. <font color = red> Semantic Search\n",
        "\n",
        "In this section, we will perform a semantic search of a query in the collections embeddings to get several top semantically similar results."
      ],
      "metadata": {
        "id": "FEiU2QCWjKLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Make a retriever\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 1})"
      ],
      "metadata": {
        "id": "7gSGq7TXrq_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a relevant query\n",
        "query = \"What is Flash attention?\""
      ],
      "metadata": {
        "id": "rrBkdbVRjo_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.get_relevant_documents(\"What is Flash attention?\")"
      ],
      "metadata": {
        "id": "v053xSR9snqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.search_type"
      ],
      "metadata": {
        "id": "B5fTNb7ZtBY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a chain"
      ],
      "metadata": {
        "id": "_lNs9zYutN8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the chain to answer questions\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(),\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "sLBZ2z07kAZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Retrieval Augmented Generation\n",
        "\n",
        "Now that we have the final top search results, we can pass it to an GPT 3.5 along with the user query and a well-engineered prompt, to generate a direct answer to the query along with citations, rather than returning whole pages/chunks."
      ],
      "metadata": {
        "id": "6A3r6nvgkv2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Cite sources\n",
        "\n",
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])"
      ],
      "metadata": {
        "id": "TLgmPqBfkTDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 1: What is Flash attention?\n",
        "query = \"What is Flash attention?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "15cJ7iNUlV6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 2: What is self attention?\n",
        "query = \"What is self-attention?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "G0_kpTv9ufsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 3: What is multi head attention?\n",
        "query = \"What is multi head attention?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "4GJ9zWycul_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 4: What does IO-aware mean?\n",
        "query = \"What does IO-aware mean?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "C4KW8Rizu2Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 5: What is tiling in flash-attention?\n",
        "query = \"What is tiling in flash-attention?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "o_71Tzc5xOVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 6: What tools can be used with toolformer?\n",
        "query = \"What tools can be used with toolformer?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "vf-vnqrfxPMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 7: What are the best retrieval augmentations for LLMs?\n",
        "query = \"What are the best retrieval augmentations for LLMs?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "jL6tMuikHrUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = input(\"What is your query about the AI research papers? \")"
      ],
      "metadata": {
        "id": "9eoJR0ckYoBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Querry 8: What are the best retrieval augmentations for LLMs?\n",
        "llm_response = qa_chain(user_query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "ns5ybbNLZ2ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deleting the Chroma Vector Database"
      ],
      "metadata": {
        "id": "wycnBknDu2pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path where chroma collections will be stored\n",
        "# chroma_data_path = '/content/drive/My Drive/GenerativeAI/MateAI/rag_assignment/ChromaDB_Data'"
      ],
      "metadata": {
        "id": "oZWX3UQhY438"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !zip -r db.zip ./db\n",
        "!zip -r ChromaDB_Data.zip '/content/drive/My Drive/GenerativeAI/MateAI/rag_assignment/ChromaDB_Data'"
      ],
      "metadata": {
        "id": "qZogCJwDu3Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To cleanup, you can delete the collection\n",
        "vectordb.delete_collection()\n",
        "vectordb.persist()\n",
        "\n",
        "# delete the directory\n",
        "!rm -rf '/content/drive/My Drive/GenerativeAI/MateAI/rag_assignment/ChromaDB_Data'"
      ],
      "metadata": {
        "id": "xNZiCuHSvBGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Starting again loading the db\n",
        "\n",
        "restart the runtime"
      ],
      "metadata": {
        "id": "AGCzTReKvGzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip db.zip\n",
        "!unzip '/content/drive/My Drive/GenerativeAI/MateAI/rag_assignment/ChromaDB_Data.zip'"
      ],
      "metadata": {
        "id": "hl3iv-79vBLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The End"
      ],
      "metadata": {
        "id": "poA5ck3tqhcT"
      }
    }
  ]
}